import sys, os, yaml
from ament_index_python.packages import get_package_share_directory, get_package_prefix
from pprint import pprint
import hashlib
from copy import deepcopy

import environment_common.convertors as C


def main():

    et_src = '/home/ros/ros2_ws/src/environment_template/'
    ec_src = '/home/ros/ros2_ws/src/environment_common/'


    # 1) load in the list of convertors
    CONVERTORS = f'{ec_src}/environment_common/included_convertors.yaml'
    with open(CONVERTORS, 'r') as f:
        convertors = yaml.safe_load(f.read())

    FARM_NAME = os.getenv('FARM_NAME')
    FIELD_NAME = os.getenv('FIELD_NAME')

    # 2) Begin loop
    while True:

        print('\n###############################')

        # 3) identify the files which are currently available
        CONFIG_DIR = os.path.join(et_src, 'config')
        walk = list(os.walk(CONFIG_DIR))
        config_files = [[w[0], [p for p in w[2] if '~' not in p and 'placeholder' not in p]] for w in walk]
        filepaths_simple = [os.path.join(d, f) for d, files in config_files if files for f in files]
        filepaths = [os.path.join(f.split('/config/')[1]) for f in filepaths_simple]

        print('=======================')
        pprint(CONFIG_DIR)
        print('-----------------------')
        pprint(filepaths)
        print('=======================')

        # 4) Log the current hash for each file in the config dir
        """
        current_hashes = {
            location/fences.kml             : cndijun9cun29u4n31o8utwneo,
            location/datum_autogen.kml      : uevwh098j0idransviutnrwe09,
            topological/network.kml         : htvun9wn97uwhg875yeqot8u98,
            topological/network_autogen.kml : hgvuhisvuh9428u5hjiou42hik,
        }
        """
        current_hashes = dict()
        for f in filepaths:
            path = os.path.join(et_src, 'config', f)
            current_hashes[f] = hashlib.md5(open(path, 'rb').read()).hexdigest()
        # 5) identify if any source files have different hashes to the
        #    logged version (autogen sourcing these must be regenerated)
        """
        conversion_sources:

            location/datum_autogen.kml:
                hash: uevwh098j0idransviutnrwe09
                data_sources:
                    - path: location/fences.kml
                      hash: cndijun9cun29u4n31o8utwneo

            topological/network_autogen.kml:
                hash: hgvuhisvuh9428u5hjiou42hik
                data_sources:
                    - path: topological/network.kml
                      hash: jrouwvh9tue3hbjoiutyoeibj0
                    - path: location/datum_autogen.yaml
                      hash: uevwh098j0idransviutnrwe09
        """
        # load the conversion log (create if not exist)
        conversion_sources_filepath = os.path.join(et_src, 'config', 'conversion_sources.yaml')
        if not os.path.exists(conversion_sources_filepath):
            with open(conversion_sources_filepath, 'w') as f:
                yaml.safe_dump({'conversion_sources':dict()}, f)
        with open(conversion_sources_filepath, 'r') as f:
            CS = yaml.safe_load(f.read())
        # identify any conversions where the data sources dont match the current_hashes list
        out_of_dates = []
        print('Autogenerated files needing regeneration:')
        for k, v in CS['conversion_sources'].items():
            for ds in v['data_sources']:
                if ds['hash'] != current_hashes[ds['path']]:
                    out_of_dates += [k]
                    print(f"| {k} with updated {ds['path']}")
                    break

        print('############################')
        pprint(current_hashes)
        print('\n\n')
        pprint(CS)
        print('\n\n')
        pprint(out_of_dates)
        print('____________________________')
        print('############################')

        # removing from filepaths will make the next step not notice them as complete
        filepaths = [f for f in filepaths if not f in out_of_dates]

        # 6) identify which of the scripts can generate new outputs
        useful = dict()
        print('\nMissing maps can be generated using:')
        for k, v in convertors['builds'].items():
            # Check if the output files already exist
            intersection = list(set(filepaths) & set(v['outputs']))
            if (not intersection):
                print(f"\n    {k}")
                [print(f"    ---| {i}") for i in v['outputs']]
                useful[k] = v

        # 7) identify which scripts have available inputs
        possible = dict()
        print('\nPossible systems to generate maps:')
        for k, v in useful.items():

            # Loop through each input requirement
            count_i = 0
            for i in v['inputs']:
                # If it is an 'either', then check intersection of sub-options
                if type(i) == dict and 'either' in list(i.keys()):
                    intersection = list(set(filepaths) & set(list(i['either'])))
                else:
                    intersection = list(set(filepaths) & set([i]))
                # If any intersections exist increment the counter
                if len(intersection) > 0:
                    count_i += 1

            # Loop through each required environment variable
            count_r = 0
            for r in v.get('requires',[]):
                if os.getenv(r):
                    count_r += 1

            # If the counter matches the total file requirements, save the convertor as possible
            if len(v.get('inputs',[])) == count_i and len(v.get('requires',[])) == count_r:
                print(f"\n    {k}")
                [print(f"    ---| {i}") for i in intersection]
                possible[k] = v

        # 8) if none can be executed, break
        if len(possible) == 0:
            print('\n\nNo more files are able to be generated with this instance. Add more data.\n')
            break

        # 9) execute any of them which can be executed
        for k,c in possible.items():
            print(f"Executing: {k}")
            args = {'src': et_src, 'files': c, 'location_name': FIELD_NAME}
            getattr(C, k).run(args)

            # 10) save the conversion to the log
            with open(conversion_sources_filepath) as f:
                conversion_sources = yaml.safe_load(f.read())
            """
            topological/network_autogen.kml:
                hash: hgvuhisvuh9428u5hjiou42hik
                data_sources:
                    - path: topological/network.kml
                      hash: jrouwvh9tue3hbjoiutyoeibj0
                    - path: location/datum_autogen.yaml
                      hash: uevwh098j0idransviutnrwe09
            """
            # calculate hash of new output files
            for o in c['outputs']:
                path = os.path.join(et_src, 'config', o)
                if os.path.exists(path):
                    current_hashes[o] = hashlib.md5(open(path, 'rb').read()).hexdigest()

            # add an entry for each output
            data_sources = []
            for i in c['inputs']:
                if isinstance(i, str): i = {'either': [i]}
                for ds in i['either']:
                    path = os.path.join(et_src, 'config', ds)
                    if os.path.exists(path):
                        data_sources.append({'path': ds, 'hash': current_hashes[ds]})

            # load in latest conversion sources (including updates from prior cycle)
            with open(conversion_sources_filepath, 'r') as f:
                CS = yaml.safe_load(f.read())
            for o in c['outputs']:
                ch = current_hashes[o]
                ds = deepcopy(data_sources)
                CS['conversion_sources'][o] = {'hash': ch, 'data_sources': ds}
            with open(conversion_sources_filepath, 'w') as f:
                yaml.safe_dump(CS, f)

        # 11) continue the main loop to check again
        print('sleeping...')
        import time
        time.sleep(5)
        continue


if __name__ == '__main__':
    main()
